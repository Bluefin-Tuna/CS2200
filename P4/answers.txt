CS 2200 Fall 2023
Project 4

Name:
GT Username: 903785867

6.1 FIFO Scheduler
Run your OS simulation with 1, 2, and 4 CPUs. Compare the total execution time of each. Is there a linear relationship between the number of CPUs and total execution time? Why or why not? Keep in mind that the execution time refers to the simulated execution time.
----------

Note that this was the result from the following simulations:
# of CPUs -> Total execution time
1 -> 67.9 s
2 -> 39.8 s
4 -> 37.0 s

The simulation results from running an OS with varying numbers of CPUs indicate a non-linear relationship between the number of CPUs and total execution time.
While doubling the CPUs from 1 to 2 nearly halved the execution time (from 67.9 seconds to 39.8 seconds), increasing from 2 to 4 CPUs resulted in a marginal decrease (to 37.0 seconds).
This non-linearity can be explained by factors such as limitations in parallelizing certain tasks, resource contention, and Amdahl's Law, which posits that speedup due to parallelization is limited by the program's sequential portion and as each process requires a specific amount of CPU burst time your limited by that.
Thus, while adding more CPUs initially reduces execution time significantly, the benefit diminishes with more CPUs due to these constraints.


6.2 Round-Robin Scheduler
Run your Round-Robin scheduler with timeslices of 800ms, 600ms, 400ms, and 200ms. Use only one CPU
for your tests. Compare the statistics at the end of the simulation. Is there a relationship between the total waiting time and timeslice length? If so, what is it? In contrast, in a real OS, the shortest timeslice possible is usually not the best choice. Why not?
----------

r 1 -r 8
Total Context Switches: 131
Total execution time: 67.9 s
Total time spent in READY state: 317.1 s

r 1 -r 6
Total Context Switches: 156
Total execution time: 67.9 s
Total time spent in READY state: 302.3 s

r 1 -r 4
Total Context Switches: 202
Total execution time: 67.9 s
Total time spent in READY state: 291.7 s

r 1 -r 2
Total Context Switches: 363
Total execution time: 67.9 s
Total time spent in READY state: 284.4 s

Based on the simulation data from the Round-Robin scheduler with different timeslices, I've noticed that as the timeslice length decreases, the total waiting time also decreases.
For example, when the timeslice was reduced from 800ms to 200ms, the total waiting time in the READY state dropped from 317.1 seconds to 284.4 seconds.
This makes sense because with shorter timeslices, processes are getting to the CPU more often.
However, this also led to a much higher number of context switches, increasing from 131 to 363 as the timeslice shortened.

Now, in a practical setting on a real OS, opting for the shortest possible timeslice isn't the best strategy.
This is because every context switch has an overhead that consumes valuable CPU time.
If the system is constantly switching contexts due to extremely short timeslices, it could end up wasting a lot of time just switching between processes rather than doing actual work.
You want to keep the timeslices short enough to maintain system responsiveness but not so short that the CPU spends more time switching than executing tasks.

6.3 Preemptive Priority Scheduler
Priority schedulers can sometimes lead to starvation among processes with lower priority. What is a way
that operating systems can mitigate starvation in a priority scheduler?
----------

To deal with the issue of starvation in a priority scheduler, I think a solid approach is to use aging.
This is where the system bumps up the priority of a process the longer it's been waiting around.
Basically, the longer a process hasn't been executed, the more its priority increases over time, ensuring it eventually gets its turn.
There's also the idea of setting a max wait time or guaranteeing a minimum amount of CPU time for each process.

6.4 Priority Inversion
Consider a non-preemptive priority scheduler. Suppose you have a high-priority process (P1) that wants to display a window on the monitor. But, the window manager is a process with low priority and will be placed at the end of the ready queue. While it is waiting to be scheduled, new medium-priority processes are likely to come in and starve the window manager process. The starvation of the window manager will also mean the starvation of P1 (the process with high priority), since P1 is waiting for the window manager to finish running. 

If we want to keep our non-preemptive priority scheduler, what edits can we make to our scheduler to ensure that the P1 can finish its execution before any of the medium priority processes finish their execution? Explain in detail the changes you would make.
----------

To tackle the priority inversion problem in our non-preemptive scheduler, I'd bring in something called priority inheritance.
If a high-priority task, letâ€™s call it P1, needs something from a lower-priority one, like the window manager, we'll bump up the window manager's priority to match P1's.
This way, the window manager gets to cut in line and let P1 carry on without getting stuck behind all those medium-priority jobs that keep showing up.
After the window manager is done, it drops back to low priority.
And just to keep things fair for those medium-priority processes, I'd make sure they get a priority kick upwards if they've been hanging around for too long.
This way, we avoid any deadlocks, and make sure everyone gets their turn without messing with the overall priority order.

